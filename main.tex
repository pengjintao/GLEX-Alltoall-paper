%\documentclass[review]{elsarticle}
\documentclass[5p,times]{elsarticle}
\usepackage{lineno,hyperref}
\usepackage{amssymb}
\usepackage{stfloats}
\modulolinenumbers[5]
\usepackage{lipsum}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{ulem}

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{GLEX-Alltoall: Overlapped Numa-aware Multi-leader Multi-port alltoall algorithm on Multi-core Supercomputer}

%% Group authors per affiliation:
\author{Jintao Peng\fnref{pjtnote}}
\author{Jie Liu\fnref{ljnote}\corref{mycorrespondingauthor}}
\author{Min Xie\fnref{xmnote}}

\address{Changsha, China}
\fntext[pjtnote]{JintaoPengCS@gmail.com}
\cortext[mycorrespondingauthor]{Corresponding author}
\fntext[ljnote]{liujie@nudt.edu.cn}
\fntext[xmnote]{xiemin@nudt.edu.cn}



\begin{abstract}
		All-to-all communication is commonly used in parallel applications like FFT. 
		In mordern supercomputers, there are multiple cores in a node.
		To optimize all-to-all communication, a typical way is gather-scatter-based alltoall which aggregate messages on each pair of nodes.
		Multiple cores, NUMAs and network endpoints bring much parallelism. 
		However, there is no method which makes uses these parallelism to improve the all-to-all communication. 
		In this paper, we introduce Overlapped Numa-aware Multi-leader Multi-port alltoall (ONMPML) collectives which explore the parallelism on network, CPU cores and overlap the intra- and inter-node communication. 
		The results show that, compared to MPI, our library achieves up to 20x speedup. 
		For application, our method achieves up to 17x speedup on peak performance for 16384 cores.
\end{abstract}

 \begin{keyword}
 Collective Communication \sep Multi-core processor \sep MPI all-to-all \sep RDMA \sep Shared Heap
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\input{./src/Introduction.tex}
\input{./src/Relatedwork.tex}
\input{./src/Environment.tex}
\input{./src/Motivation.tex}
\input{./src/Algorithm.tex}
\input{./src/Modelingresult.tex}
\input{./src/Implementation.tex}
\input{./src/Experiment.tex}



\bibliography{mybibfile}

\end{document}