\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Related Work}{}% 2
\BOOKMARK [1][-]{section.3}{Experimental Platforms}{}% 3
\BOOKMARK [1][-]{section.4}{Motivation}{}% 4
\BOOKMARK [1][-]{section.5}{Gather-Scatter-based Multi-leader all-to-all algorithm}{}% 5
\BOOKMARK [2][-]{subsection.5.1}{Leader-based All-to-All Collective \(L-a2a\)}{section.5}% 6
\BOOKMARK [2][-]{subsection.5.2}{Multi-port Multi-leader All-to-all Collective \(MPML\)}{section.5}% 7
\BOOKMARK [2][-]{subsection.5.3}{NUMA-aware MPML all-to-all collective \(NMPML\)}{section.5}% 8
\BOOKMARK [2][-]{subsection.5.4}{Overlapping Intra-node and Inter-node Communication of NMPML \(ONMPML\)}{section.5}% 9
\BOOKMARK [1][-]{section.6}{Modeling the performance of all-to-all algorithms on multi-core supercomputer}{}% 10
\BOOKMARK [2][-]{subsection.6.1}{The parameters of the model}{section.6}% 11
\BOOKMARK [2][-]{subsection.6.2}{Overhead of direct pairwise all-to-all}{section.6}% 12
\BOOKMARK [2][-]{subsection.6.3}{Overhead of leader-based all-to-all \(L-a2a\)}{section.6}% 13
\BOOKMARK [2][-]{subsection.6.4}{Overhead of Multi-port Multi-leader a2a \(MPML\)}{section.6}% 14
\BOOKMARK [2][-]{subsection.6.5}{Overhead of NUMA-aware MPML all-to-all \(NMPML\) }{section.6}% 15
\BOOKMARK [2][-]{subsection.6.6}{Overhead of ONMPML}{section.6}% 16
\BOOKMARK [1][-]{section.7}{Implementation with POSIX shared memory and RDMA}{}% 17
\BOOKMARK [2][-]{subsection.7.1}{Impementation of Intra-node Communication}{section.7}% 18
\BOOKMARK [2][-]{subsection.7.2}{Impementation of Inter-node Communication}{section.7}% 19
\BOOKMARK [1][-]{section.8}{Evaluation}{}% 20
\BOOKMARK [2][-]{subsection.8.1}{Microbenchmark Evalutation}{section.8}% 21
\BOOKMARK [3][-]{subsubsection.8.1.1}{Microbenchmark Result on HPC-A}{subsection.8.1}% 22
\BOOKMARK [3][-]{subsubsection.8.1.2}{Microbenchmark Result on HPC-B}{subsection.8.1}% 23
\BOOKMARK [2][-]{subsection.8.2}{HPCC-FFT Evaluation}{section.8}% 24
