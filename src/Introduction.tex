\section{Introduction}
%对于很多并行应用而言，全局的通信往往成为限制其扩展性的关键点。
%聚合通信作为普遍使用的全局通信，在各种应用种广泛使用。
%all-to-all通信作为聚合通信之一，在FFT和图计算应用中有重要应用。
%然而all-to-all通信在任意两个进程之间都需要发消息。每次并行规模翻倍，all-to-all通信量翻4倍。
%另一方面，网络吞吐率和节点数量的增加则是线性增长。
%这给大规模all-to-all通信带来巨大的挑战。

%典型的方法是在节点内进行聚合消息然后在节点间传输数据。
%简单的说就是将MPI_alltoall替换为n次gather+节点间alltoall+n次scatter。
%该方法对于小消息非常有效。
%通过这种方法消息数量降低节点内进程数量的平方倍。
%在当前超级计算机结构下，有4种并行性有助于优化该节点感知的all-to-all的通信性能：
%多个网络端口使得网卡可以同时处理多个通信请求。
%多个内存控制器/NUMA/内存通道，带来的访存并行性。
%多核心带来的通信请求构造数据Gather和Scatter的并行性。
%节点内和节点间数据传输的并行性。
%但是据我们所知，没有一种节点感知的all-to-all方法充分发掘了这四种并行性。

%在节点内，优化节点感知的all-to-all通信算法在节点内的典型方法有共享内存。
%该方法能够使得节点内通信绕过网卡进行。
%此外，内核辅助的“零拷贝”技术以及共享堆的也是常见的方法。
%这些方法能够降低一次点内的内存拷贝次数。
%同时也有一些方法考虑到了NUMA/UMA中的cache利用效率的问题。
%但是这些方法并没有考虑到随着超级计算机的发展，节点间通信并不会比节点内通信慢太多。
%现代超级计算机普遍装备着多核心，多numa的处理器。
%典型的方法通常没有将这其中的并行性发掘出来。

%本文提出了一种多leader的节点感知all-to-all通信库。
%利用多个核心进行本地转置，发掘多核并行。
%使用多核CPU核心同时进行Gather和Scatter操作，发掘多NUMA并行访存。
%利用多个网络端口同时进行消息传输，发掘网络并行行。
%将节点内和节点间数据传输重叠起来，提升大消息通信性能。
%该通信库在节点内使用共享堆作为Gather和Scatter操作的基础。
%在节点间使用RDMA进行网络通信。
%实验表明，对于16384个进程，我们的通信库对比MPI可以实现最大10X的性能提升。从小消息到大消息平均可提升4.16倍。

Many parallel applications may suffer from global communication.
Especially for communication-intensive applications, their time-to-solution and scalablily may be affected by global communication. 
Message Passing Interface (MPI) provides a set of commonly used collective communication.
MPI\_Alltoall is one of the collective communication where each process will send a different message to all processes.
It is broadly used in some parallel applications like Fast Fourier Transform (FFT) \cite{mehta2021parallel} and some graph algorithms like MapReduce \cite{plimpton2011mapreduce} and Breadth-first search (BFS) \cite{ueno2012highly}.
However, each time we double the processes, the communication workload of all-to-all is quadrupled.
On mordern supercomputers, network throughput has a linear relationship with the number of nodes.
This brings great challenges to design large-scale all-to-all collectives.

For multiple-core supercomputers, an effective way is node-aware all-to-all method \cite{sistare1999optimization}.
A p (p=N times M, M is \#processes on a node) processes all-to-all is replaeced by a N node inter-node all-to-all. Which is: N-1 times intra-node gather + local transpose + inter-node transpose + N-1 times intra-node gather.
This method is very effective for small messages.
Because, compared to original method, a node-aware all-to-all reduce the number of inter-node messages from $(M^N)^2$ to $N^2$ times.
The size of the message is increased by $M^2$ times, which can effective use of the network bandwidth.
On the current supercomputer, there may be 20-64 CPU cores in a node.
The single message size can be scaled 400-4096 times.
In production environment, node-aware all-to-all may more efficient than Bruck's algorithm.


However, the gathering/scattering overhead on node-aware all-to-all make it hard to be scaled to larger messages.
Multiple CPU cores, NUMA and network endpoints brings 4 kinds of parallism to optimize a node-aware all-to-all method:
\begin{enumerate}[(1)]
\item Multiple network endpoints can simultaneously process multiple communication requests.
\item Processes in different NUMA can simultaneously access it local memory without contention.
\item Multiple processes can simultaneously gather/scatter data and compose communication requests.
\item Inter-node communication can be overlapped with intra-node communication.
\end{enumerate}
As we known, no methods combine these parallism togather to improve a node-aware all-to-all collective communication. 

% A typical intra-node communication is based on shared memory.
% Processes create a shared memory at initialization stage, the data is fistly copied from senders buffer into the shared memory buffer, then copied out to the receivers buffer.
% Two data copy needed to transer a intra-node message.
% To reduce the memory copy times, kernel-assisted communication, shared heaps, and thread-based MPI are proposed.

In this paper, we proposed Overlapped NUMA-aware Multi-port Multi-leader (ONMPML) method to scale the node-aware all-to-all collective.
ONMPML using multiple leaders on different NUMA to gather/scatter data, compose communication requests, and transpose local matrix and open the different network endpoints to do inter-node all-to-all.
ONMPML explore the parallelism existing in mordern multi-core processor with NUMA memory architecture and multi-port network.
For intra-node gather/sactter, we implemented a shared-heap-based remote accessible memory to support multiple leader to gather/scatter simutaneously.
Inter-node communication is implemented on Remote Direct Memory Access (RDMA) which provide high throughput and low latency.
The results show that, compared to MPI\_alltoall, our implementation achieves up to 18x speedup and 4x speedup on average. 